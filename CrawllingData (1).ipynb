{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CrawllingData.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RvkRuCuMgUj"
      },
      "source": [
        "# **Import Library Yang Dibutuhkan**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-9ZuQmgH0Gr"
      },
      "source": [
        "!pip install tweet-preprocessor\n",
        "!pip install google_trans_new\n",
        "!pip install googletrans\n",
        "!pip install langdetect\n",
        "!pip install tweepy\n",
        "!pip install textblob\n",
        "!pip install wordcloud\n",
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qi10YmX_NNkq"
      },
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from googletrans import Translator\n",
        "from datetime import timedelta, datetime\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from types import SimpleNamespace\n",
        "import matplotlib.pyplot as plt \n",
        "from tweepy import OAuthHandler\n",
        "from textblob import TextBlob\n",
        "import preprocessor as p\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import tweepy\n",
        "import csv\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jN6BBHRBLIYC"
      },
      "source": [
        "# **Pengambilan Data (Crawling Data)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDeq7yxMHJsB"
      },
      "source": [
        "access_token = \"758101339486289920-KvautH0nfyRWC5jQ7R0IP58kSIw8Kiy\"\n",
        "access_token_secret = \"EeuruuYkXyxpHB56cnhyVC5fJOZwqhEFjlc4CH9igjg91\"\n",
        "consumer_key = \"vyOy6wjwOhrxRMyIxFgPPtiW8\"\n",
        "consumer_secret = \"XLEncikSzjje4jNc3iWKzfMSTC4UCWjVheJEzQi0ETG1ZRXZYj\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZ4nEUQPH3EB",
        "outputId": "2811a498-9168-4343-d456-b89eaeade00e"
      },
      "source": [
        "auth = OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "def scraptweets(search_words, date_since, date_until):\n",
        "\n",
        "    db_tweets = pd.DataFrame(columns=['username', 'tweetcreatedts', 'text'])\n",
        "\n",
        "    tweets = tweepy.Cursor(\n",
        "                    api.search, q=search_words, lang=\"id\", \n",
        "                    since=date_since, until=date_until,  tweet_mode='extended').items(1000)\n",
        "\n",
        "    tweet_list = [tweet for tweet in tweets]\n",
        "\n",
        "    for tweet in tweet_list:\n",
        "        username = tweet.user.screen_name\n",
        "        tweetcreatedts = tweet.created_at\n",
        "\n",
        "        try:\n",
        "            text = tweet.retweeted_status.full_text\n",
        "        except AttributeError:\n",
        "            text = tweet.full_text\n",
        "\n",
        "        ith_tweet = [username, tweetcreatedts, text]\n",
        "\n",
        "        db_tweets.loc[len(db_tweets)] = ith_tweet\n",
        "    \n",
        "    print('Proses Scrapping Selesai Dengan Jumlah Data', len(db_tweets))\n",
        "    filename = 'covid_vaccine_tweets.csv'\n",
        "    db_tweets.to_csv(filename, index=False)\n",
        "\n",
        "today = datetime.today().strftime(\"%Y-%m-%d\")\n",
        "last_week = datetime.today() - timedelta(7)\n",
        "last_week = last_week.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "search_words = \"#vaksin OR #vaksincovid19 OR #vaksincovid OR #Vaksinsinovac\"\n",
        "date_since = last_week\n",
        "date_until = today\n",
        "\n",
        "scraptweets(search_words, date_since, date_until)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proses Scrapping Selesai Dengan Jumlah Data 708\n"
          ]
        }
      ]
    }
  ]
}